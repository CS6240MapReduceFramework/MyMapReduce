package client;

import java.io.*;
import java.lang.*;

import textsock.TextSocket;
import java.util.*;
import java.util.zip.GZIPInputStream;
import com.amazonaws.auth.*;
import com.amazonaws.services.s3.*;
import com.amazonaws.services.s3.model.*;
import com.amazonaws.AmazonClientException;
import com.amazonaws.AmazonServiceException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.CopyObjectRequest;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.Upload;
import com.amazonaws.services.s3.transfer.MultipleFileDownload;
import com.amazonaws.services.s3.transfer.Copy;
import com.amazonaws.services.s3.transfer.MultipleFileUpload;
import org.apache.commons.io.FileUtils;

public class WebClient {

	private static Properties prop = new Properties();
	private static int partCounter = 1;
	private static int sizeOfFiles = 1024 * 1024 * 128; // in MB
	private static byte[] buffer = new byte[sizeOfFiles];

	//FilePaths
	private static String inputFilePath;  //To retrieve Input files in s3://inputBucket/inputFolder
	private static String outputFilePath; //To write output files s3://outputBucket/outputFolder
	private static String inputBucket; //S3 Input bucket name 
	private static String inputPath; //path within input Bucket
	private static String outputBucket; //S3 Output Bucket name
	private static String outputPath; //path within the outputbucket

	// These local folders are deleted after the files are uploaded to s3
	private static String temporaryDataDivision; //Local copy of divided files stored here
	private static String temporaryMergedFiles; // Local copy of merged files with temp files generated by Mapper task
	private static String temporaryFilesFromMapper; //Local copy of temporary files from Mapper task
	private static String localFinalOutput; //final output of the map reduce program will be written here

	static {
		try {
			//Properties file should be within src folder
			prop.load(new FileInputStream("../config.properties"));
		} catch (Exception e) {
			System.out.println(e.getMessage());
		}
	}

	public static ArrayList<String> outputRecords = new ArrayList<String>();
	static private AWSCredentials credentials = new BasicAWSCredentials(prop.getProperty("AWSAccessKeyId"), prop.getProperty("AWSSecretKey"));
	static private AmazonS3 s3Client = new AmazonS3Client(credentials);
	static private TransferManager tx = new TransferManager(credentials);

	/*
	 * Fetches the list of file names in s3://<inputBucket>/input folder
	 * Input Arguments: nil
	 * Returns: ArrayList<String>  - List of file names in the given s3 folder
	 */
	public static ArrayList<String> getFilesList(String bucketfolder) {
		ArrayList<String> files = new ArrayList<String>();
		try {
			ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(inputBucket).withPrefix(bucketfolder + "/");
			ObjectListing objectListing;

			do {
				objectListing = s3Client.listObjects(listObjectsRequest);

				for (S3ObjectSummary objectSummary : objectListing.getObjectSummaries()) {
					if (!objectSummary.getKey().equals(bucketfolder + "/"))
						files.add(objectSummary.getKey());
				}

				listObjectsRequest.setMarker(objectListing.getNextMarker());
			} while (objectListing.isTruncated());

		} catch (AmazonServiceException ase) {
			System.out.println("AmazonServiceException");
			ase.printStackTrace();
		} catch (AmazonClientException ace) {
			System.out.println("AmazonClientException");
		} finally {
			return files;
		}
	}



	/**
	 * @param key	A file path in S3
	 * @param bucketFolder	BucketName in S3
	 */
	public static void handleSingleFile(String key, String bucketFolder) {

		divideSingleFile(key);
		uploadPartsToS3(bucketFolder);
		bakMainS3File(key);
	}

	/**
	 * Fetches the file extension and divides the file into chunks of 128MB
	 * @param key A file path in S3
	 */
	public static void divideSingleFile(String key) {

		try {
			String fileExtension = getFileExtension(key);
			if (fileExtension.equals("gz")) {
				divideCompressedFile(key);
			} else {
				divideRegularFile(key);
			}
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	/**
	 * Retrieves the extension of a given file
	 * @param fileName - A Filename 
	 * @return String - extension
	 */
	public static String getFileExtension(String fileName) {
		return fileName.substring(fileName.lastIndexOf(".") + 1);
	}

	public static void divideCompressedFile(String key) throws IOException {

		S3Object object = s3Client.getObject(new GetObjectRequest(inputBucket, key));
		GZIPInputStream gis = new GZIPInputStream(object.getObjectContent());
		BufferedInputStream bis = new BufferedInputStream(gis);

		String fileName = key.substring(key.lastIndexOf("/") + 1, key.lastIndexOf("."));
		writeToFiles(bis, fileName);

		bis.close();
		gis.close();
	}

	/**
	 * Divides the file in given S3 file path and writes the file in local file system
	 * @param key - A file path in S3
	 * @throws IOException 
	 */
	public static void divideRegularFile(String key) throws IOException {

		S3Object object = s3Client.getObject(new GetObjectRequest(inputBucket, key));
		BufferedInputStream bis = new BufferedInputStream(object.getObjectContent());
		String fileName = key.substring(key.lastIndexOf("/") + 1);
		writeToFiles(bis, fileName);
		bis.close();
	}

	/**
	 * Reads the data in given S3 file path and writes the chunks into temporaryDataDivision path in local file system
	 * @param bis -	BufferedInputStream of a file in S3
	 * @param fileName	- File to be divided in S3
	 * @throws IOException 
	 */
	public static void writeToFiles(BufferedInputStream bis, String fileName) throws IOException {

		File dataDir = new File(temporaryDataDivision);
		dataDir.mkdir();
		int tmp = 0;
		while ((tmp = bis.read(buffer)) > 0) {
			File newFile = new File(String.format("%03d", partCounter++) + "-" + fileName);
			try (FileOutputStream out = new FileOutputStream(temporaryDataDivision + "/" + newFile)) {
				out.write(buffer, 0, tmp);
			}
		}
	}

	/**
	 * Uploads the divided chunks of input data into given folder in S3
	 * @param bucketFolder - A folder in S3 to where the part-files are written into
	 */
	public static void uploadPartsToS3(String bucketFolder) {

		File uploadDir = new File(temporaryDataDivision);
		File[] listOfFiles = uploadDir.listFiles();
		for (int i = 0; i < listOfFiles.length; i++) {
			s3Client.putObject(new PutObjectRequest(inputBucket, bucketFolder + "/" + listOfFiles[i].getName(), listOfFiles[i]));
			listOfFiles[i].delete();
		}
		uploadDir.delete();
	}


	/**
	 * Backup the data in S3://<inputBucket>/<folder> to S3://<inputBucket>/BakInputFiles folder
	 * @param key - Input file name in S3
	 */
	public static void bakMainS3File(String key) {

		try {
			//Bakup of input file
			CopyObjectRequest copyObjRequest = new CopyObjectRequest(
					inputBucket, key, inputBucket, "BakInputFiles/" + key);

			Copy cp = tx.copy(copyObjRequest);
			cp.waitForCompletion();
			s3Client.deleteObject(new DeleteObjectRequest(inputBucket, key));

		} catch (AmazonServiceException ase) {
			System.out.println("Error Message:    " + ase.getMessage());
		} catch (AmazonClientException ace) {
			System.out.println("Error Message: " + ace.getMessage());
		} catch (InterruptedException ioe) {
			System.out.println("error message in threadl.sleep");
		}

	}

	/**
	 * 
	 * @param instancesCount - No of server instances
	 * @param ips	- IPs of server instances
	 * @param bucketfolder - A folder in S3 to which the divided files are written
	 * @param move - boolean to whether move files or copy the files
	 */
	public static void divideFilesInS3(int instancesCount, String[] ips, String bucketfolder, Boolean move) {

		ArrayList<String> files = getFilesList(bucketfolder);


		for(String file: files)
		{
			handleSingleFile(file, bucketfolder);
		}		
		files = getFilesList(bucketfolder);

		int chunk_size = files.size() / instancesCount;
		int remaining_chunk_size = files.size() % instancesCount;

		System.out.println("Total number of files (after division) - " + files.size());

		int instance = 0;
		System.out.println("Copying files to respective instance folders...");

		for (int i = 0; i < files.size(); i++) {
			try {
				// Copying object
				CopyObjectRequest copyObjRequest = new CopyObjectRequest(
						inputBucket, files.get(i), outputBucket, ips[instance] + "/" + files.get(i));

				Copy cp = tx.copy(copyObjRequest);
				cp.waitForCompletion();
				if (move)
					s3Client.deleteObject(new DeleteObjectRequest(outputBucket, files.get(i)));
				instance++;
				if (instance == instancesCount)
					instance = 0;

			} catch (AmazonServiceException ase) {
				System.out.println("Error Message:    " + ase.getMessage());
			} catch (AmazonClientException ace) {
				System.out.println("Error Message: " + ace.getMessage());
			} catch (InterruptedException ioe) {
				System.out.println("error message in threadl.sleep");
			}
		}
	}

	/**
	 * Sends given signal to all the server instances through socket connections
	 * @param connections 	- A connection object through which client is connected with server
	 * @throws IOException
	 */
	public static void startPhase(TextSocket[] connections, String signal) throws IOException {
		for (TextSocket connection : connections)
			connection.putln(signal);
	}

	/**
	 * Waits for a signal from all the servers
	 * @param connections - An array of connection objects through which client is connected with server
	 * @throws IOException
	 */
	public static void waitForPhaseCompletion(TextSocket[] connections) throws IOException {
		for (TextSocket connection : connections)
			connection.getln();
	}

	/**
	 * Close connection when a connection receives a signal (REDUCER_COMPLETE)
	 * @param connections
	 * @throws IOException
	 */
	public static void closeConnections(TextSocket[] connections) throws IOException {
		System.out.println("REDUCER_COMPLETE signal receieved. Closing all connections");
		for (TextSocket connection : connections) {
			connection.getln();
			connection.close();
		}
		System.out.println("All connections closed successfully!");

	}

	/**
	 * Downloads all the part files in S3 created by Reducers 
	 * @param ips- IP addresses of the server instances
	 * @param outputBucket - The folder in S3 from where the part files are to be downloaded from
	 */
	public static void downloadOutputPartFilesFromS3(String[] ips, String outputBucket) {
		try {

			File localFolder = new File(localFinalOutput);
			if (!localFolder.exists())
				localFolder.mkdirs();

			MultipleFileDownload md = tx.downloadDirectory(outputBucket, outputPath, localFolder);
			md.waitForCompletion();

			System.out.println("All output part files from "+outputPath+" downloaded");

		} catch (AmazonServiceException ase) {
			System.out.println("AmazonServiceException");
			ase.printStackTrace();
		} catch (AmazonClientException ace) {
			System.out.println("AmazonClientException");
			ace.printStackTrace();
		} catch (Exception e) {
			System.out.println("Excepton");
			e.printStackTrace();
		}
	}

	/**
	 * Download temp files from all S3://<inputBucket>/<IP> paths in S3
	 * @param ips - IP addresses of all the server instances
	 */
	public static void downloadIntermediateFilesFromS3(String[] ips) {
		try {
			File localFolder = new File(temporaryFilesFromMapper);
			if (!localFolder.exists())
				localFolder.mkdirs();

			for (int i = 0; i < ips.length; i++) {
				MultipleFileDownload md = tx.downloadDirectory(outputBucket, ips[i] + "/" + temporaryFilesFromMapper, localFolder);
				md.waitForCompletion();
			}
			System.out.println("All temp files from all instances downloaded");
		} catch (AmazonServiceException ase) {
			System.out.println("AmazonServiceException");
			ase.printStackTrace();
		} catch (AmazonClientException ace) {
			System.out.println("AmazonClientException");
			ace.printStackTrace();
		} catch (Exception e) {
			System.out.println("Excepton");
			e.printStackTrace();
		}
	}

	/**
	 * Upload Files from given local folder to given bucket folder in given bucket name
	 * @param localfolder - Path of files to be uploaded
	 * @param bucketName	- S3 bucket name to which files are uploaded
	 * @param bucketFolder - Path in S3 to which files are uploaded to.
	 */
	public static void uploadMergedFilesToS3(String localfolder, String bucketName, String bucketFolder) {

		try {
			File local = new File(localfolder);
			System.out.println("Uploading files to S3://"+bucketName+"/"+bucketFolder+" from "+localfolder);

			MultipleFileUpload mu = tx.uploadDirectory(bucketName, bucketFolder, local, true);
			mu.waitForCompletion();
			s3Client.deleteObject(new DeleteObjectRequest(bucketName, "output/.DS_Store"));

		} catch (AmazonServiceException ase) {
			ase.printStackTrace();
		} catch (AmazonClientException ace) {
			ace.printStackTrace();
		} catch (Exception e) {
			e.printStackTrace();
		}

		FileUtils.deleteQuietly(new File(temporaryMergedFiles));

	}

	/** Merges the files with same file names in given folders
	 * @throws IOException
	 */
	public static void mergeIntermediateFilesFromS3() throws IOException {

		File tempfiles = new File(temporaryFilesFromMapper);

		if (!tempfiles.exists())
			System.out.println(temporaryFilesFromMapper+" folder not found");

		FileWriter fileWriter;
		BufferedWriter bufferedWriter;

		for (File instance : tempfiles.listFiles()) {

			if (instance.getName().contains("DS_Store")) {
				continue;
			}

			//Open each instance folder
			File subFolder = new File(instance.getAbsolutePath() + "/tempFiles");

			//Iterate over all tempfiles in each isntance folder
			for (File tempFile : subFolder.listFiles()) {

				if (instance.getName().contains("DS_Store")) {
					continue;
				}
				//Open tempfile for read
				FileReader tmpFileReader = new FileReader(tempFile);
				BufferedReader bufferedReader = new BufferedReader(tmpFileReader);
				String line = "";

				//Open file in parent folder for writing the tempfile
				File fdir = new File(temporaryMergedFiles);
				if (!fdir.exists())
					fdir.mkdirs();

				File f = new File(temporaryMergedFiles + "/" + tempFile.getName());

				if (!f.exists())
					f.createNewFile();

				//create writer to copy all the content from tempfile to one tempfile in parent folder(true as second property)
				fileWriter = new FileWriter(f, true);
				bufferedWriter = new BufferedWriter(fileWriter);

				//read lines from temp instance file
				while ((line = bufferedReader.readLine()) != null) {

					//write to one file
					bufferedWriter.write(line + "\n");
					bufferedWriter.flush();
				}

				//close writer
				bufferedWriter.close();
				//close reader
				bufferedReader.close();
			}
		}
		//delete all temp files
		FileUtils.deleteQuietly(new File(temporaryFilesFromMapper));
	}

	/**
	 * Delete given file paths in S3
	 * @param bucketName - S3 bucket name
	 * @param folderToDelete - Path in S3
	 */
	public static void deleteEmptyFolderS3(String bucketName, String folderToDelete) {
		try {
			s3Client.deleteObject(new DeleteObjectRequest(bucketName, folderToDelete));
		} catch (AmazonServiceException ase) {
			ase.printStackTrace();
		} catch (AmazonClientException ace) {
			ace.printStackTrace();
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	/**
	 * @param args	- s3://<inputBucket>/input s3://<outputBucket>/output
	 * @throws IOException
	 */
	public static void main(String[] args) throws IOException {

		if (args.length != 2) {
			System.out.println("Not enough arguments passed");
			System.out.println("Usage: WebClient s3://<inputBucket>/input s3://<outputBucket>/output");
			System.exit(1);
		}

		//static FilePaths for entire Program usage
		String inputFilePath = args[0];
		String outputFilePath = args[1];

		String[] inputDataLocationSplits = inputFilePath.split("//")[1].split("/");
		inputBucket = inputDataLocationSplits[0];
		inputPath = inputDataLocationSplits[1];

		String[] outputDataLocatonSplits = outputFilePath.split("//")[1].split("/");
		outputBucket = outputDataLocatonSplits[0];
		outputPath = outputDataLocatonSplits[1];

		System.out.println("Input Bucket - " + inputBucket);
		System.out.println("Output Bucket - " + outputBucket);
		System.out.println("Input path in s3 - " + inputPath);
		System.out.println("Output path in s3 - " + outputPath);

		temporaryDataDivision = "data";
		temporaryMergedFiles = "merged";
		temporaryFilesFromMapper = "tempFiles";
		localFinalOutput="FinalOutput";

		//Read the instances.txt file
		System.out.println("Reading instances.txt file");
		Scanner sc = new Scanner(new File("instances.txt"));
		int instances_num = Integer.parseInt(sc.nextLine());
		System.out.println("Instances count: " + instances_num);

		int count = 0;

		String ips_ports[] = new String[instances_num];
		String instanceIp = "";
		TextSocket[] connections = new TextSocket[instances_num];

		while (sc.hasNextLine()) {
			String[] line = sc.nextLine().split(";");
			String instance_id = line[0];
			instanceIp = line[1];

			String portNum = line[2];
			ips_ports[count] = instanceIp + "_" + portNum;

			int port = Integer.parseInt(line[2]);
			System.out.println("Establishing connection to: " + instanceIp + " and port - " + port);
			TextSocket conn = new TextSocket(instanceIp, port);

			System.out.println("Connection established..");
			connections[count] = conn;

			count++;
			conn.putln(inputBucket);
			conn.putln(instanceIp);
			conn.putln(portNum);
			conn.putln(outputBucket);

			System.out.println("Program started on :" + instanceIp);
		}

		divideFilesInS3(instances_num, ips_ports, inputPath, false);
		startPhase(connections,"MAPPER_START");
		waitForPhaseCompletion(connections);
		downloadIntermediateFilesFromS3(ips_ports);
		mergeIntermediateFilesFromS3();
		uploadMergedFilesToS3(temporaryMergedFiles, outputBucket, temporaryMergedFiles);
		divideFilesInS3(instances_num, ips_ports, temporaryMergedFiles, true);
		deleteEmptyFolderS3(outputBucket, temporaryMergedFiles);
		startPhase(connections,"REDUCER_START");
		closeConnections(connections);
		downloadOutputPartFilesFromS3(ips_ports, outputBucket);

		System.exit(0);
	}

}

